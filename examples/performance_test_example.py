"""
æ€§èƒ½æµ‹è¯•ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•å¯¹ OpenAPI MCP Server è¿›è¡Œæ€§èƒ½æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•ã€‚
åŒ…æ‹¬è´Ÿè½½æµ‹è¯•ã€å¹¶å‘æµ‹è¯•ã€å†…å­˜ä½¿ç”¨æµ‹è¯•ç­‰ã€‚
"""

import asyncio
import json
import time
from dataclasses import dataclass
from datetime import datetime
from statistics import mean, median
from typing import Any

import aiohttp
import psutil
from fastapi import FastAPI
from openapi_mcp.client import OpenApiMcpClient
from openapi_mcp.transport.http import HttpMcpTransport

from openapi_mcp import OpenApiMcpServer


# åˆ›å»ºæ€§èƒ½æµ‹è¯•ç”¨çš„åº”ç”¨
def create_performance_test_app() -> FastAPI:
	"""åˆ›å»ºæ€§èƒ½æµ‹è¯•åº”ç”¨"""
	app = FastAPI(
		title='Performance Test API',
		version='1.0.0',
		description='ç”¨äºæ€§èƒ½æµ‹è¯•çš„ API',
	)

	# ç®€å•çš„ç«¯ç‚¹
	@app.get('/simple')
	async def simple_endpoint():
		"""ç®€å•ç«¯ç‚¹ - å¿«é€Ÿå“åº”"""
		return {'message': 'Hello World', 'timestamp': time.time()}

	# ä¸­ç­‰å¤æ‚åº¦ç«¯ç‚¹
	@app.get('/medium')
	async def medium_endpoint():
		"""ä¸­ç­‰å¤æ‚åº¦ç«¯ç‚¹ - éœ€è¦ä¸€äº›è®¡ç®—"""
		# æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
		result = sum(i * i for i in range(100))
		return {'result': result, 'timestamp': time.time()}

	# å¤æ‚ç«¯ç‚¹
	@app.get('/complex')
	async def complex_endpoint():
		"""å¤æ‚ç«¯ç‚¹ - å¤§é‡æ•°æ®å¤„ç†"""
		# æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
		data = []
		for i in range(1000):
			item = {
				'id': i,
				'name': f'item_{i}',
				'value': i * 2,
				'metadata': {
					'created': time.time(),
					'tags': [f'tag_{j}' for j in range(5)],
				},
			}
			data.append(item)
		return {'data': data, 'total': len(data)}

	# å¸¦å‚æ•°çš„ç«¯ç‚¹
	@app.get('/params/{item_id}')
	async def params_endpoint(item_id: int, query: str = ''):
		"""å¸¦å‚æ•°ç«¯ç‚¹"""
		return {'item_id': item_id, 'query': query, 'timestamp': time.time()}

	# è¿”å›å¤§é‡æ•°æ®çš„ç«¯ç‚¹
	@app.get('/large-data')
	async def large_data_endpoint():
		"""è¿”å›å¤§é‡æ•°æ®çš„ç«¯ç‚¹"""
		data = []
		for i in range(5000):
			data.append(
				{
					'id': i,
					'content': 'x' * 100,  # 100å­—ç¬¦çš„å†…å®¹
					'metadata': {'index': i, 'hash': hash(f'item_{i}')},
				}
			)
		return {'data': data, 'size': len(data)}

	return app


@dataclass
class PerformanceMetrics:
	"""æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""

	endpoint: str
	total_requests: int
	successful_requests: int
	failed_requests: int
	avg_response_time: float
	min_response_time: float
	max_response_time: float
	median_response_time: float
	p95_response_time: float
	p99_response_time: float
	requests_per_second: float
	error_rate: float
	total_duration: float


@dataclass
class SystemMetrics:
	"""ç³»ç»ŸæŒ‡æ ‡æ•°æ®ç±»"""

	cpu_percent: float
	memory_percent: float
	memory_used_mb: float
	memory_available_mb: float
	disk_io_read_mb: float
	disk_io_write_mb: float
	network_io_sent_mb: float
	network_io_recv_mb: float


class PerformanceTester:
	"""æ€§èƒ½æµ‹è¯•å™¨"""

	def __init__(self, base_url: str = 'http://localhost:8000'):
		self.base_url = base_url
		self.mcp_url = f'{base_url}/mcp'
		self.session: aiohttp.ClientSession | None = None

	async def __aenter__(self):
		"""å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
		self.session = aiohttp.ClientSession()
		return self

	async def __aexit__(self, exc_type, exc_val, exc_tb):
		"""å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
		if self.session:
			await self.session.close()

	async def test_endpoint_performance(
		self,
		endpoint: str,
		concurrent_users: int = 10,
		requests_per_user: int = 50,
		duration_seconds: int | None = None,
	) -> PerformanceMetrics:
		"""æµ‹è¯•å•ä¸ªç«¯ç‚¹çš„æ€§èƒ½"""
		print(f'\nğŸš€ æµ‹è¯•ç«¯ç‚¹: {endpoint}')
		print(f'   å¹¶å‘ç”¨æˆ·: {concurrent_users}, æ¯ç”¨æˆ·è¯·æ±‚æ•°: {requests_per_user}')

		url = f'{self.base_url}{endpoint}'
		response_times = []
		successful_requests = 0
		failed_requests = 0
		start_time = time.time()

		# åˆ›å»ºå¹¶å‘ä»»åŠ¡
		async def user_session():
			nonlocal successful_requests, failed_requests
			user_response_times = []

			for _i in range(requests_per_user):
				request_start = time.time()
				try:
					async with self.session.get(url) as response:
						if response.status == 200:
							await response.text()  # è¯»å–å“åº”å†…å®¹
							successful_requests += 1
						else:
							failed_requests += 1
				except Exception:
					failed_requests += 1

				request_time = time.time() - request_start
				user_response_times.append(request_time)

				# æ£€æŸ¥æ˜¯å¦è¶…æ—¶
				if duration_seconds and (time.time() - start_time) > duration_seconds:
					break

			return user_response_times

		# æ‰§è¡Œå¹¶å‘æµ‹è¯•
		tasks = [user_session() for _ in range(concurrent_users)]
		results = await asyncio.gather(*tasks)

		# æ”¶é›†æ‰€æœ‰å“åº”æ—¶é—´
		for user_times in results:
			response_times.extend(user_times)

		total_duration = time.time() - start_time
		total_requests = successful_requests + failed_requests

		# è®¡ç®—æŒ‡æ ‡
		if response_times:
			avg_response_time = mean(response_times)
			min_response_time = min(response_times)
			max_response_time = max(response_times)
			median_response_time = median(response_times)

			# è®¡ç®—ç™¾åˆ†ä½æ•°
			sorted_times = sorted(response_times)
			p95_index = int(len(sorted_times) * 0.95)
			p99_index = int(len(sorted_times) * 0.99)
			p95_response_time = (
				sorted_times[p95_index]
				if p95_index < len(sorted_times)
				else max_response_time
			)
			p99_response_time = (
				sorted_times[p99_index]
				if p99_index < len(sorted_times)
				else max_response_time
			)
		else:
			avg_response_time = min_response_time = max_response_time = 0
			median_response_time = p95_response_time = p99_response_time = 0

		requests_per_second = (
			total_requests / total_duration if total_duration > 0 else 0
		)
		error_rate = (
			(failed_requests / total_requests * 100) if total_requests > 0 else 0
		)

		metrics = PerformanceMetrics(
			endpoint=endpoint,
			total_requests=total_requests,
			successful_requests=successful_requests,
			failed_requests=failed_requests,
			avg_response_time=avg_response_time * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
			min_response_time=min_response_time * 1000,
			max_response_time=max_response_time * 1000,
			median_response_time=median_response_time * 1000,
			p95_response_time=p95_response_time * 1000,
			p99_response_time=p99_response_time * 1000,
			requests_per_second=requests_per_second,
			error_rate=error_rate,
			total_duration=total_duration,
		)

		return metrics

	async def test_mcp_performance(
		self, tool_name: str, concurrent_users: int = 5, requests_per_user: int = 20
	) -> PerformanceMetrics:
		"""æµ‹è¯• MCP å·¥å…·çš„æ€§èƒ½"""
		print(f'\nğŸ”§ æµ‹è¯• MCP å·¥å…·: {tool_name}')

		async def mcp_request():
			"""æ‰§è¡Œ MCP è¯·æ±‚"""
			start_time = time.time()
			try:
				transport = HttpMcpTransport(self.mcp_url)
				client = OpenApiMcpClient(transport)
				await transport.connect()

				# è°ƒç”¨å·¥å…·
				if tool_name == 'search_endpoints':
					await client.call_tool(tool_name, {'query': 'test'})
				elif tool_name == 'generate_examples':
					await client.call_tool(
						tool_name, {'endpoint_path': '/test', 'method': 'GET'}
					)
				else:
					await client.call_tool(tool_name, {})

				await transport.disconnect()
				return True, time.time() - start_time

			except Exception as e:
				print(f'    MCP è¯·æ±‚å¤±è´¥: {e}')
				return False, time.time() - start_time

		# æ‰§è¡Œå¹¶å‘æµ‹è¯•
		response_times = []
		successful_requests = 0
		failed_requests = 0
		start_time = time.time()

		async def user_session():
			nonlocal successful_requests, failed_requests
			user_response_times = []

			for _ in range(requests_per_user):
				success, request_time = await mcp_request()
				user_response_times.append(request_time)

				if success:
					successful_requests += 1
				else:
					failed_requests += 1

			return user_response_times

		tasks = [user_session() for _ in range(concurrent_users)]
		results = await asyncio.gather(*tasks)

		for user_times in results:
			response_times.extend(user_times)

		total_duration = time.time() - start_time
		total_requests = successful_requests + failed_requests

		# è®¡ç®—æŒ‡æ ‡ï¼ˆåŒä¸Šï¼‰
		if response_times:
			avg_response_time = mean(response_times)
			min_response_time = min(response_times)
			max_response_time = max(response_times)
			median_response_time = median(response_times)

			sorted_times = sorted(response_times)
			p95_index = int(len(sorted_times) * 0.95)
			p99_index = int(len(sorted_times) * 0.99)
			p95_response_time = (
				sorted_times[p95_index]
				if p95_index < len(sorted_times)
				else max_response_time
			)
			p99_response_time = (
				sorted_times[p99_index]
				if p99_index < len(sorted_times)
				else max_response_time
			)
		else:
			avg_response_time = min_response_time = max_response_time = 0
			median_response_time = p95_response_time = p99_response_time = 0

		requests_per_second = (
			total_requests / total_duration if total_duration > 0 else 0
		)
		error_rate = (
			(failed_requests / total_requests * 100) if total_requests > 0 else 0
		)

		return PerformanceMetrics(
			endpoint=f'MCP:{tool_name}',
			total_requests=total_requests,
			successful_requests=successful_requests,
			failed_requests=failed_requests,
			avg_response_time=avg_response_time * 1000,
			min_response_time=min_response_time * 1000,
			max_response_time=max_response_time * 1000,
			median_response_time=median_response_time * 1000,
			p95_response_time=p95_response_time * 1000,
			p99_response_time=p99_response_time * 1000,
			requests_per_second=requests_per_second,
			error_rate=error_rate,
			total_duration=total_duration,
		)

	def collect_system_metrics(self) -> SystemMetrics:
		"""æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
		# CPU ä½¿ç”¨ç‡
		cpu_percent = psutil.cpu_percent(interval=1)

		# å†…å­˜ä½¿ç”¨æƒ…å†µ
		memory = psutil.virtual_memory()
		memory_percent = memory.percent
		memory_used_mb = memory.used / (1024 * 1024)
		memory_available_mb = memory.available / (1024 * 1024)

		# ç£ç›˜ I/O
		disk_io = psutil.disk_io_counters()
		disk_io_read_mb = disk_io.read_bytes / (1024 * 1024) if disk_io else 0
		disk_io_write_mb = disk_io.write_bytes / (1024 * 1024) if disk_io else 0

		# ç½‘ç»œ I/O
		network_io = psutil.net_io_counters()
		network_io_sent_mb = network_io.bytes_sent / (1024 * 1024) if network_io else 0
		network_io_recv_mb = network_io.bytes_recv / (1024 * 1024) if network_io else 0

		return SystemMetrics(
			cpu_percent=cpu_percent,
			memory_percent=memory_percent,
			memory_used_mb=memory_used_mb,
			memory_available_mb=memory_available_mb,
			disk_io_read_mb=disk_io_read_mb,
			disk_io_write_mb=disk_io_write_mb,
			network_io_sent_mb=network_io_sent_mb,
			network_io_recv_mb=network_io_recv_mb,
		)


class LoadTestScenario:
	"""è´Ÿè½½æµ‹è¯•åœºæ™¯"""

	def __init__(self, name: str, description: str):
		self.name = name
		self.description = description
		self.metrics: list[PerformanceMetrics] = []

	def add_metrics(self, metrics: PerformanceMetrics):
		"""æ·»åŠ æµ‹è¯•æŒ‡æ ‡"""
		self.metrics.append(metrics)

	def get_summary(self) -> dict[str, Any]:
		"""è·å–æµ‹è¯•æ€»ç»“"""
		if not self.metrics:
			return {}

		total_requests = sum(m.total_requests for m in self.metrics)
		total_successful = sum(m.successful_requests for m in self.metrics)
		total_failed = sum(m.failed_requests for m in self.metrics)

		avg_rps = mean(m.requests_per_second for m in self.metrics)
		avg_response_time = mean(m.avg_response_time for m in self.metrics)
		avg_error_rate = mean(m.error_rate for m in self.metrics)

		return {
			'scenario_name': self.name,
			'description': self.description,
			'total_requests': total_requests,
			'successful_requests': total_successful,
			'failed_requests': total_failed,
			'overall_error_rate': avg_error_rate,
			'avg_requests_per_second': avg_rps,
			'avg_response_time_ms': avg_response_time,
			'test_count': len(self.metrics),
		}


class PerformanceReporter:
	"""æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""

	def __init__(self):
		self.scenarios: list[LoadTestScenario] = []
		self.system_metrics: list[SystemMetrics] = []

	def add_scenario(self, scenario: LoadTestScenario):
		"""æ·»åŠ æµ‹è¯•åœºæ™¯"""
		self.scenarios.append(scenario)

	def add_system_metrics(self, metrics: SystemMetrics):
		"""æ·»åŠ ç³»ç»ŸæŒ‡æ ‡"""
		self.system_metrics.append(metrics)

	def generate_report(self, output_file: str = 'performance_report.json') -> str:
		"""ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
		report = {
			'test_metadata': {
				'generated_at': datetime.now().isoformat(),
				'total_scenarios': len(self.scenarios),
				'test_duration_minutes': len(self.system_metrics)
				if self.system_metrics
				else 0,
			},
			'scenarios': [scenario.get_summary() for scenario in self.scenarios],
			'detailed_metrics': [],
			'system_metrics': [],
		}

		# æ·»åŠ è¯¦ç»†æŒ‡æ ‡
		for scenario in self.scenarios:
			for metrics in scenario.metrics:
				report['detailed_metrics'].append(
					{
						'scenario': scenario.name,
						'endpoint': metrics.endpoint,
						'total_requests': metrics.total_requests,
						'successful_requests': metrics.successful_requests,
						'failed_requests': metrics.failed_requests,
						'avg_response_time_ms': metrics.avg_response_time,
						'min_response_time_ms': metrics.min_response_time,
						'max_response_time_ms': metrics.max_response_time,
						'median_response_time_ms': metrics.median_response_time,
						'p95_response_time_ms': metrics.p95_response_time,
						'p99_response_time_ms': metrics.p99_response_time,
						'requests_per_second': metrics.requests_per_second,
						'error_rate': metrics.error_rate,
						'total_duration': metrics.total_duration,
					}
				)

		# æ·»åŠ ç³»ç»ŸæŒ‡æ ‡
		for i, metrics in enumerate(self.system_metrics):
			report['system_metrics'].append(
				{
					'timestamp': i,  # ç®€åŒ–çš„æ—¶é—´æˆ³
					'cpu_percent': metrics.cpu_percent,
					'memory_percent': metrics.memory_percent,
					'memory_used_mb': metrics.memory_used_mb,
					'memory_available_mb': metrics.memory_available_mb,
					'disk_io_read_mb': metrics.disk_io_read_mb,
					'disk_io_write_mb': metrics.disk_io_write_mb,
					'network_io_sent_mb': metrics.network_io_sent_mb,
					'network_io_recv_mb': metrics.network_io_recv_mb,
				}
			)

		# ä¿å­˜æŠ¥å‘Š
		with open(output_file, 'w', encoding='utf-8') as f:
			json.dump(report, f, indent=2, ensure_ascii=False)

		print(f'\nğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}')
		return output_file

	def print_summary(self):
		"""æ‰“å°æµ‹è¯•æ€»ç»“"""
		print('\n' + '=' * 80)
		print('ğŸ“Š æ€§èƒ½æµ‹è¯•æ€»ç»“æŠ¥å‘Š')
		print('=' * 80)

		for scenario in self.scenarios:
			summary = scenario.get_summary()
			print(f'\nğŸ¯ åœºæ™¯: {summary["scenario_name"]}')
			print(f'   æè¿°: {summary["description"]}')
			print(f'   æ€»è¯·æ±‚æ•°: {summary["total_requests"]}')
			print(f'   æˆåŠŸè¯·æ±‚: {summary["successful_requests"]}')
			print(f'   å¤±è´¥è¯·æ±‚: {summary["failed_requests"]}')
			print(f'   é”™è¯¯ç‡: {summary["overall_error_rate"]:.2f}%')
			print(f'   å¹³å‡ RPS: {summary["avg_requests_per_second"]:.2f}')
			print(f'   å¹³å‡å“åº”æ—¶é—´: {summary["avg_response_time_ms"]:.2f}ms')

		if self.system_metrics:
			print('\nğŸ’» ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ:')
			avg_cpu = mean(m.cpu_percent for m in self.system_metrics)
			avg_memory = mean(m.memory_percent for m in self.system_metrics)
			max_memory = max(m.memory_used_mb for m in self.system_metrics)

			print(f'   å¹³å‡ CPU ä½¿ç”¨ç‡: {avg_cpu:.1f}%')
			print(f'   å¹³å‡å†…å­˜ä½¿ç”¨ç‡: {avg_memory:.1f}%')
			print(f'   å³°å€¼å†…å­˜ä½¿ç”¨: {max_memory:.1f}MB')


async def run_performance_tests():
	"""è¿è¡Œæ€§èƒ½æµ‹è¯•"""
	print('ğŸš€ OpenAPI MCP æ€§èƒ½æµ‹è¯•å¥—ä»¶')
	print('=' * 60)

	# åˆ›å»ºæµ‹è¯•åº”ç”¨
	app = create_performance_test_app()

	# é›†æˆ MCP æœåŠ¡å™¨
	mcp_server = OpenApiMcpServer(app)
	mcp_server.mount('/mcp')

	# åœ¨åå°å¯åŠ¨æœåŠ¡å™¨
	import threading

	import uvicorn

	def run_server():
		uvicorn.run(app, host='0.0.0.0', port=8000, log_level='warning')

	server_thread = threading.Thread(target=run_server, daemon=True)
	server_thread.start()

	# ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
	await asyncio.sleep(3)

	# åˆ›å»ºæ€§èƒ½æµ‹è¯•å™¨å’ŒæŠ¥å‘Šç”Ÿæˆå™¨
	async with PerformanceTester() as tester:
		reporter = PerformanceReporter()

		# æµ‹è¯•åœºæ™¯1: åŸºç¡€ç«¯ç‚¹æ€§èƒ½æµ‹è¯•
		print('\nğŸ“‹ åœºæ™¯1: åŸºç¡€ç«¯ç‚¹æ€§èƒ½æµ‹è¯•')
		scenario1 = LoadTestScenario('åŸºç¡€ç«¯ç‚¹æµ‹è¯•', 'æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„åŸºç¡€ç«¯ç‚¹æ€§èƒ½')

		endpoints = [
			('/simple', 'ç®€å•ç«¯ç‚¹'),
			('/medium', 'ä¸­ç­‰å¤æ‚åº¦ç«¯ç‚¹'),
			('/complex', 'å¤æ‚ç«¯ç‚¹'),
			('/large-data', 'å¤§æ•°æ®ç«¯ç‚¹'),
		]

		for endpoint, description in endpoints:
			metrics = await tester.test_endpoint_performance(
				endpoint, concurrent_users=10, requests_per_user=50
			)
			scenario1.add_metrics(metrics)

			print(
				f'  âœ… {description}: RPS={metrics.requests_per_second:.1f}, '
				f'å“åº”æ—¶é—´={metrics.avg_response_time:.1f}ms, '
				f'é”™è¯¯ç‡={metrics.error_rate:.1f}%'
			)

		reporter.add_scenario(scenario1)

		# æµ‹è¯•åœºæ™¯2: å¹¶å‘å‹åŠ›æµ‹è¯•
		print('\nğŸ“‹ åœºæ™¯2: å¹¶å‘å‹åŠ›æµ‹è¯•')
		scenario2 = LoadTestScenario('å¹¶å‘å‹åŠ›æµ‹è¯•', 'æµ‹è¯•é«˜å¹¶å‘æƒ…å†µä¸‹çš„æ€§èƒ½è¡¨ç°')

		for concurrent_users in [20, 50, 100]:
			metrics = await tester.test_endpoint_performance(
				'/medium', concurrent_users=concurrent_users, requests_per_user=20
			)
			scenario2.add_metrics(metrics)

			print(
				f'  âœ… {concurrent_users} å¹¶å‘ç”¨æˆ·: RPS={metrics.requests_per_second:.1f}, '
				f'å“åº”æ—¶é—´={metrics.avg_response_time:.1f}ms, '
				f'é”™è¯¯ç‡={metrics.error_rate:.1f}%'
			)

		reporter.add_scenario(scenario2)

		# æµ‹è¯•åœºæ™¯3: MCP å·¥å…·æ€§èƒ½æµ‹è¯•
		print('\nğŸ“‹ åœºæ™¯3: MCP å·¥å…·æ€§èƒ½æµ‹è¯•')
		scenario3 = LoadTestScenario('MCPå·¥å…·æµ‹è¯•', 'æµ‹è¯• MCP å·¥å…·çš„æ€§èƒ½')

		mcp_tools = ['search_endpoints', 'generate_examples']

		for tool in mcp_tools:
			try:
				metrics = await tester.test_mcp_performance(
					tool, concurrent_users=5, requests_per_user=10
				)
				scenario3.add_metrics(metrics)

				print(
					f'  âœ… {tool}: RPS={metrics.requests_per_second:.1f}, '
					f'å“åº”æ—¶é—´={metrics.avg_response_time:.1f}ms, '
					f'é”™è¯¯ç‡={metrics.error_rate:.1f}%'
				)
			except Exception as e:
				print(f'  âŒ {tool}: æµ‹è¯•å¤±è´¥ - {e}')

		reporter.add_scenario(scenario3)

		# æµ‹è¯•åœºæ™¯4: æŒä¹…è´Ÿè½½æµ‹è¯•
		print('\nğŸ“‹ åœºæ™¯4: æŒä¹…è´Ÿè½½æµ‹è¯•')
		scenario4 = LoadTestScenario('æŒä¹…è´Ÿè½½æµ‹è¯•', 'æµ‹è¯•ç³»ç»Ÿåœ¨æŒç»­è´Ÿè½½ä¸‹çš„ç¨³å®šæ€§')

		# æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
		for i in range(5):  # 5æ¬¡æµ‹è¯•ï¼Œæ¯æ¬¡é—´éš”10ç§’
			if i > 0:
				await asyncio.sleep(10)

			system_metrics = tester.collect_system_metrics()
			reporter.add_system_metrics(system_metrics)

			metrics = await tester.test_endpoint_performance(
				'/medium', concurrent_users=20, requests_per_user=30
			)
			scenario4.add_metrics(metrics)

			print(
				f'  âœ… ç¬¬{i + 1}è½®: RPS={metrics.requests_per_second:.1f}, '
				f'CPU={system_metrics.cpu_percent:.1f}%, '
				f'å†…å­˜={system_metrics.memory_percent:.1f}%'
			)

		reporter.add_scenario(scenario4)

		# ç”ŸæˆæŠ¥å‘Š
		report_file = reporter.generate_report()
		reporter.print_summary()

		# æ€§èƒ½åŸºå‡†æµ‹è¯•
		print('\nğŸ¯ æ€§èƒ½åŸºå‡†å¯¹æ¯”')
		print('-' * 40)

		for scenario in reporter.scenarios:
			if scenario.metrics:
				best_rps = max(m.requests_per_second for m in scenario.metrics)
				best_response = min(m.avg_response_time for m in scenario.metrics)
				worst_response = max(m.avg_response_time for m in scenario.metrics)

				print(f'{scenario.name}:')
				print(f'   æœ€é«˜ RPS: {best_rps:.1f}')
				print(f'   æœ€å¿«å“åº”: {best_response:.1f}ms')
				print(f'   æœ€æ…¢å“åº”: {worst_response:.1f}ms')

		return report_file


def main():
	"""ä¸»å‡½æ•°"""
	print('ğŸ”§ OpenAPI MCP æ€§èƒ½æµ‹è¯•ç¤ºä¾‹')
	print('=' * 60)

	# å®‰è£…ä¾èµ–æ£€æŸ¥
	required_packages = ['aiohttp', 'psutil']
	missing_packages = []

	for package in required_packages:
		try:
			__import__(package)
		except ImportError:
			missing_packages.append(package)

	if missing_packages:
		print(f'âŒ ç¼ºå°‘å¿…éœ€çš„åŒ…: {", ".join(missing_packages)}')
		print('è¯·è¿è¡Œ: pip install aiohttp psutil')
		return

	# è¿è¡Œæ€§èƒ½æµ‹è¯•
	try:
		report_file = asyncio.run(run_performance_tests())
		print(f'\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼æŠ¥å‘Šæ–‡ä»¶: {report_file}')
	except Exception as e:
		print(f'âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}')
		import traceback

		traceback.print_exc()


if __name__ == '__main__':
	main()
